<!DOCTYPE html>
<html lang="zxx">

<head>
  <meta charset="utf-8">
  <title>THUHCSI</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- ** Plugins Needed for the Project ** -->
  <!-- Bootstrap -->
  <link rel="stylesheet" href="plugins/bootstrap/bootstrap.min.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="plugins/slick/slick.css">
  <!-- themefy-icon -->
  <link rel="stylesheet" href="plugins/themify-icons/themify-icons.css">
  <!-- animation css -->
  <link rel="stylesheet" href="plugins/animate/animate.css">
  <!-- aos -->
  <link rel="stylesheet" href="plugins/aos/aos.css">
  <!-- venobox popup -->
  <link rel="stylesheet" href="plugins/venobox/venobox.css">

  <!-- Main Stylesheet -->
  <link href="css/style.css" rel="stylesheet">

  <!--Favicon-->
  <link rel="shortcut icon" href="images/favicon.jpg" type="image/x-icon">
  <link rel="icon" href="images/favicon.jpg" type="image/x-icon">

</head>

<body>
  <!-- preloader start --
  <div class="preloader">
    <img src="images/hcsi-pre.gif" alt="preloader" width="500px">
  </div>
  <!-- preloader end -->

<!-- header -->
<header class="fixed-top header">
  <!-- navbar -->
  <div class="navigation w-100">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark p-0">
        <!-- logo -->
        <a class="navbar-brand" href="index.html"><img src="images/logo.png" alt="logo" width="250"></a>
        <button class="navbar-toggler rounded-0" type="button" data-toggle="collapse" data-target="#navigation" aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <!-- menu -->
        <div class="collapse navbar-collapse" id="navigation">
          <ul class="navbar-nav ml-auto text-center">
            <!-- about -->
            <li class="nav-item dropdown view @@about">
              <a class="nav-link dropdown-toggle" href="#" id="navbarAbout" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                About
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarAbout">
                <a class="dropdown-item" href="labintro.html">Introduction</a>
                <a class="dropdown-item" href="researches.html">Research Areas</a>
                <a class="dropdown-item" href="collaborators.html">Collaborators</a>
              </div>
            </li>
            <!-- members -->
            <li class="nav-item dropdown view @@members">
              <a class="nav-link dropdown-toggle" href="#" id="navbarMembers" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Members
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarMembers">
                <a class="dropdown-item" href="zywu.html">Director</a>
                <a class="dropdown-item" href="members.html">Students</a>
                <a class="dropdown-item" href="alumni.html">Alumni</a>
                <a class="dropdown-item" href="honors.html">Honors</a>
              </div>
            </li>
            <!-- news -->
            <li class="nav-item @@news">
              <a class="nav-link" href="news.html">News</a>
            </li>
            <!-- research -->
            <li class="nav-item dropdown view active">
              <a class="nav-link dropdown-toggle" href="#" id="navbarResearch" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Research
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarResearch">
                <a class="dropdown-item active" href="publications.html">Publications</a>
                <a class="dropdown-item" href="patents.html">Patents</a>
                <a class="dropdown-item" href="projects.html">Projects</a>
              </div>
            </li>
            <!-- awards -->
            <li class="nav-item @@awards">
              <a class="nav-link" href="awards.html">Awards</a>
            </li>
            <!-- demos -->
            <li class="nav-item @@demos">
              <a class="nav-link" href="demos.html">Demos</a>
            </li>
          </ul>
        </div>
        <!-- /menu -->
      </nav>
    </div>
  </div>
  <!-- /navbar -->
</header>
<!-- /header -->

<!-- page title -->
<section class="page-title-section overlay" data-background="images/backgrounds/hcsi-gather.jpg">
  <div class="container">
    <div class="row">
      <div class="col-md-8">
        <ul class="list-inline custom-breadcrumb">
          <li class="list-inline-item"><p class="h2 text-primary font-secondary">Publications</p></li>
          <li class="list-inline-item text-white h3 font-secondary"></li>
        </ul>
        <p class="text-lighten">&nbsp;</p>
      </div>
    </div>
  </div>
</section>
<!-- /page title -->

<!-- papers -->
<section class="section">
  <div class="container">
    <!-- paper category navigation bar -->
    <div class="row">
      <div class="col-12">
        <!-- nav tab -->
        <div class="border-bottom">
          <ul class="nav nav-pills text-center">
            <li class="navi-item">
              <a class="nav-link active" href="#year" data-toggle="pill">
                  by year
              </a>
            </li>
            <li class="navi-item">
              <a class="nav-link" href="#area" data-toggle="pill">
                  by area
              </a>
            </li>
            <li class="navi-item">
              <a class="nav-link" href="#cat" data-toggle="pill">
                  by category
              </a>
            </li>
          </ul>
        </div>
        <!-- nav tab content -->
        <div class="tab-content" id="pills-tabContent">
          <div class="tab-pane fade show active" id="year">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2022">2022</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2021">2021</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2020">2020</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2019">2019</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2018">2018</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2017">2017</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2016">2016</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2015">2015</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2014">2014</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2013">2013</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2012">2012</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2011">2011</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2010">2010</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2009">2009</li>
              </ul>
            </div>
          </div>
          <div class="tab-pane fade" id="area">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ss">Speech Synthesis</li>
                <li class="list-inline-item px-2 mb-3" data-filter="sr">Speech Recognition</li>
                <li class="list-inline-item px-2 mb-3" data-filter="sv">Speaker Recognition</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ssp">Speech Signal Processing</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ac">Affective Computing</li>
                <li class="list-inline-item px-2 mb-3" data-filter="mslp">Multimodal Speech and Language Processing</li>
              </ul>
            </div>
          </div>
          <div class="tab-pane fade" id="cat">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="jnl">Journal</li>
                <li class="list-inline-item px-2 mb-3"  data-filter="cf">Conference</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- papers list -->
    <div class="filtr-container">
      <!-- paper -->
      <div data-category="2022,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Shiyin Kang, Helen Meng,
            "Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7922-7926. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312199470, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747438">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-expressive-tts-hierarchical-context/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jingbei Li, Yi Meng, Chenyi Li, Zhiyong Wu, Helen Meng, Chao Weng, Dan Su,
            "Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-Based Multi-Modal Context Modeling,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7917-7921. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747837">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-conversational-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Liyang Chen, Zhiyong Wu, Jun Ling, Runnan Li, Xu Tan, Sheng Zhao,
            "Transformer-S2A: Robust and Efficient Speech-to-Animation,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7247-7251. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198574, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747495">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-Transformer-S2A/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xintao Zhao, Feng Liu, Changhe Song, Zhiyong Wu, Shiyin Kang, Deyi Tuo, Helen Meng,
            "Disentangling Content and Fine-Grained Prosody Information Via Hybrid ASR Bottleneck Features for Voice Conversion,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7022-7026. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198907, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747625">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-hybrid-bottleneck-vc/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xueyuan Chen, Changhe Song, Yixuan Zhou, Zhiyong Wu, Changbin Chen, Zhongqin Wu, Helen Meng,
            "A Character-Level Span-Based Model for Mandarin Prosodic Structure Prediction,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7602-7606. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198495, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747315">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/SpanPSP">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/SpanPSP/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Wenlin Dai, Changhe Song, Xiang Li, Zhiyong Wu, Huashan Pan, Xiulin Li, Helen Meng,
            "An End-to-End Chinese Text Normalization Model Based on Rule-Guided Flat-Lattice Transformer,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7122-7126. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198496, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747316">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/FlatTN">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jingbei Li, Yi Meng, Zhiyong Wu, Helen Meng, Qiao Tian, Yuping Wang, Yuxuan Wang,
            "Neufa: Neural Network Based End-to-End Forced Alignment with Bidirectional Attention Mechanism,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 8007-8011. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198218, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747085">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/NeuFA">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Wenxuan Ye, Shaoguang Mao, Frank Soong, Wenshan Wu, Yan Xia, Jonathan Tien, Zhiyong Wu,
            "An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6827-6831. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312199246, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746604">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-MDD-APL/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ssp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, Helen Meng,
            "FullSubNet+: Channel Attention Fullsubnet with Complex Spectrograms for Speech Enhancement,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7857-7861. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747888">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/FullSubNet-plus">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://hit-thusz-rookiecj.github.io/fullsubnet-plus.github.io/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin Wu, Shoukang Hu, Zhiyong Wu, Xunying Liu, Helen Meng,
            "Neural Architecture Search for Speech Emotion Recognition,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6902-6906. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198129, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746155">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,sv" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Haibin Wu, Po-Chun Hsu, Ji Gao, Shanshan Zhang, Shen Huang, Jian Kang, Zhiyong Wu, Helen Meng, Hung-Yi Lee,
            "Adversarial Sample Detection for Speaker Verification by Neural Vocoders,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 236-240. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198990, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746900">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/hbwu-ntu/spot-adv-by-vocoder">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,jnl,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin WU, Yuewen CAO, Hui LU, Songxiang LIU, Disong WANG, Zhiyong WU, Xunying LIU, Helen MENG,
            "Speech Emotion Recognition Using Sequential Capsule Networks,"
            <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>,
            vol. 29, pp. 3280-3291. IEEE, October, 2021.
            <span class="text-lighten">(SCI: WOS:000714713700004, EI: 20214311082562, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9576634">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,jnl,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin WU, Yuewen CAO, Hui LU, Songxiang LIU, Shiyin KANG, Zhiyong WU, Xunying LIU, Helen MENG,
            "Exemplar-Based Emotive Speech Synthesis,"
            <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>,
            vol. 29, pp. 874-886. IEEE, January, 2021.
            <span class="text-lighten">(SCI: WOS:000619310400001, EI: 20210409830187, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9328288">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yingmei GUO, Linjun SHOU, Jian PEI, Ming GONG, Mingxing XU, Zhiyong WU, Daxin JIANG,
            "Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding,"
            [in] <i>Proc. 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>,
            pp. 1-12. Punta Cana, Dominican Republic, November 7-11, 2021.
            <span class="text-lighten">(EI: 20221411909706, THU-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2109.01583">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yaohua BU, Tianyi MA, Weijun LI, Hang ZHOU, Jia JIA, Shengqi CHEN, Kaiyuan XU, Dachuan SHI, Haozhe WU, Zhihan YANG, Kun LI, Zhiyong WU, Yuanchun SHI, Xiaobo LU, Ziwei LIU,
            "PTeacher: A Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback,"
            [in] <i>Proc. 2021 CHI Conference on Human Factors in Computing Systems (CHI)</i>,
            pp. 1-14. Yokohama, Japan, May 8-13, 2021.
            <span class="text-lighten">(EI: 20212210439123, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/3411764.3445490">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://hcsi.cs.tsinghua.edu.cn/demo/CHI21-BUYAOHUA.mp4">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Suping ZHOU, Jia JIA, Zhiyong WU, Zhihan YANG, Yanfeng WANG, Wei CHEN, Fanbo MENG, Shuo HUANG, Jialie SHEN, Xiaochuan WANG,
            "Inferring Emotion from Large-Scale Internet Voice Data: A Semi-supervised Curriculum Augmentation based Deep Learning Approach,"
            [in] <i>Proc. the 35th AAAI Conference on Artificial Intelligence (AAAI)</i>,
            pp. 6039-6047. Virtual, Online, February 2-9, 2021.
            <span class="text-lighten">(EI: 20222012114882, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://hcsi.cs.tsinghua.edu.cn/Paper/Paper21/AAAI21-ZHOUSUPING.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2019,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Runnan LI, Zhiyong WU, Jia JIA, Yaohua BU, Sheng ZHAO, Helen MENG,
            "Towards Discriminative Representation Learning for Speech Emotion Recognition,"
            [in] <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI)</i>,
            pp. 5060-5066. Macao, China, August 10-16, 2019.
            <span class="text-lighten">(EI: 20194607696464, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.ijcai.org/proceedings/2019/0703.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/IJCAI2019-DRL4SER">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2019,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yishuang NING, Sheng HE, Zhiyong WU, Chunxiao XING, Liangjie ZHANG,
            "A Review of Deep Learning Based Speech Synthesis,"
            <i>Applied Sciences-Basel</i>,
            vol. 9, no. 19, pp. 4050. MDPI, September, 2019.
            <span class="text-lighten">(SCI: WOS:000496258100108)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.mdpi.com/2076-3417/9/19/4050">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2018,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Runnan LI, Zhiyong WU, Jia JIA, Jingbei LI, Wei CHEN, Helen MENG,
            "Inferring User Emotive State Changes in Realistic Human-Computer Conversational Dialogs,"
            [in] <i>Proc. ACM Multimedia Conference (ACM MM)</i>,
            pp. 136-144. Seoul, Korea, October 22-26, 2018.
            <span class="text-lighten">(EI: 20185006246269, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/3240508.3240575">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2018,jnl,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Kun LI, Shaoguang MAO, Xu LI, Zhiyong WU, Helen MENG,
            "Automatic Lexical Stress and Pitch Accent Detection for L2 English Speech using Multi-Distribution Deep Neural Networks,"
            <i>Speech Communication (Speech Com)</i>,
            vol. 96, pp. 28-36. Elsevier, February, 2018.
            <span class="text-lighten">(SCI: WOS:000424723700003, EI: 20174704448303, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.sciencedirect.com/science/article/pii/S0167639315300637">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2017,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yishuang NING, Jia JIA, Zhiyong WU, Runnan LI, Yongsheng AN, Yanfeng WANG, Helen MENG,
            "Multi-task Deep Learning for User Intention Understanding in Speech Interaction Systems,"
            [in] <i>Proc. the 31th AAAI Conference on Artificial Intelligence (AAAI)</i>,
            pp. 161-167. San Francisco, USA, February 4-9, 2017.
            <span class="text-lighten">(EI: 20174104242835, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ojs.aaai.org/index.php/AAAI/article/view/10493">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Yishuang NING, Xiao ZANG, Jia JIA, Fanbo MENG, Helen MENG, Lianhong CAI,
            "Generating Emphatic Speech with Hidden Markov Model for Expressive Speech Synthesis,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 74, no. 22, pp. 9909-9925. Springer, July, 2015.
             <span class="text-lighten">(SCI: WOS:000364019400005, EI: 20143600027913, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-014-2164-2">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Kai ZHAO, Xixin WU, Xinyu LAN, Helen MENG,
            "Acoustic to Articulatory Mapping with Deep Neural Network,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 74, no. 22, pp. 9889-9907. Springer, August, 2015.
            <span class="text-lighten">(SCI: WOS:000364019400004, EI: 20143600014973, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-014-2183-z">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,cf,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Qi LYU, Zhiyong WU, Jun ZHU,
            "Polyphonic Music Modelling with LSTM-RTRBM,"
            [in] <i>Proc. ACM Multimedia Conference (ACM MM)</i>,
            pp. 991-994. Brisbane, Australia, October 26-30, 2015.
            <span class="text-lighten">(EI: 20161602252616, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/2733373.2806383">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,cf,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Qi LYU, Zhiyong WU, Jun ZHU, Helen MENG,
            "Modelling High-dimensional Sequences with LSTM-RTRBM: Application to Polyphonic Music Generation,"
            [in] <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI)</i>,
            pp. 4138-4139. Buenos Aires, Argentina, July 25-31, 2015.
            <span class="text-lighten">(EI: 20155101693661, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.ijcai.org/Proceedings/15/Papers/582.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2014,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Fanbo MENG, Zhiyong WU, Jia JIA, Helen MENG, Lianhong CAI,
            "Synthesizing English Emphatic Speech for Multimodal Corrective Feedback in Computer-Aided Pronunciation Training,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 73, no. 1, pp. 463-489. Springer, September, 2014.
            <span class="text-lighten">(SCI: WOS:000342418700022, EI: 20143600046713, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-013-1601-y">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2014,jnl,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jia JIA, Zhiyong WU, Shen ZHANG, Helen MENG, Lianhong CAI,
            "Head and Facial Gestures Synthesis using PAD Model for an Expressive Talking Avatar,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 73, no. 1, pp. 439-461. Springer, September, 2014.
            <span class="text-lighten">(SCI: WOS:000342418700023, EI: 20143600046670, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-013-1604-8">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2009,jnl,ac,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Helen M. MENG, Hongwu YANG, Lianhong CAI,
            "Modeling the Expressivity of Input Text Semantics for Chinese Text-to-Speech Synthesis in a Spoken Dialog System,"
            <i>IEEE Transaction on Audio, Speech and Language Processing (TASLP)</i>,
            vol. 17, no. 8, pp. 1567-1577. IEEE, November, 2009.
            <span class="text-lighten">(SCI: WOS:000268903600010, EI: 20093612281690, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/4926212">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- /papers -->

<!-- footer -->
<footer>
  <!-- footer content -->
  <div class="footer bg-footer section border-bottom">
    <div class="container">
      <div class="row">
        <div class="col-lg-4 col-md-4 col-sm-5 mb-5 mb-sm-0 text-center">
          <!-- logo -->
          <a class="logo-footer" href="index.html"><img class="img-fluid mb-0" src="images/QRCode.png" alt="QR code"></a>
        </div>
        <!-- contact -->
        <div class="col-lg-8 col-md-8 col-sm-7 mb-0">
          <h4 class="text-lightblue mb-2">Location</h4>
          <p class="text-white mb-4">Room 1701, Information Building, Tsinghua Campus, The University Town, Shenzhen 518055, China <br>
            深圳市南山区西丽大学城清华校区信息大楼1701
          </p>
          <h4 class="text-lightblue mb-2">Follow Us</h4>
          <p class="text-white mb-0">Scan QR code to follow us on WeChat <br>
            扫码关注实验室微信公众号
          </p>
        </div>
      </div>
    </div>
  </div>
  <!-- copyright -->
  <div class="copyright py-4 bg-footer">
    <div class="container">
      <div class="row">
        <div class="col-sm-7 text-sm-left text-center">
          <p class="mb-0">Copyright &copy;
            <script>
              var CurrentYear = new Date().getFullYear()
              document.write(CurrentYear)
            </script>,
            im1eon @ thuhcsi</p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!-- /footer -->

<!-- jQuery -->
<script src="plugins/jQuery/jquery.min.js"></script>
<!-- Bootstrap JS -->
<script src="plugins/bootstrap/bootstrap.min.js"></script>
<!-- slick slider -->
<script src="plugins/slick/slick.min.js"></script>
<!-- aos -->
<script src="plugins/aos/aos.js"></script>
<!-- venobox popup -->
<script src="plugins/venobox/venobox.min.js"></script>
<!-- filter -->
<script src="plugins/filterizr/jquery.filterizr.min.js"></script>

<!-- Main Script -->
<script src="js/script.js"></script>

</body>
</html>
