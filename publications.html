<!DOCTYPE html>
<html lang="zxx">

<head>
  <meta charset="utf-8">
  <title>THUHCSI</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- ** Plugins Needed for the Project ** -->
  <!-- Bootstrap -->
  <link rel="stylesheet" href="plugins/bootstrap/bootstrap.min.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="plugins/slick/slick.css">
  <!-- themefy-icon -->
  <link rel="stylesheet" href="plugins/themify-icons/themify-icons.css">
  <!-- animation css -->
  <link rel="stylesheet" href="plugins/animate/animate.css">
  <!-- aos -->
  <link rel="stylesheet" href="plugins/aos/aos.css">
  <!-- venobox popup -->
  <link rel="stylesheet" href="plugins/venobox/venobox.css">

  <!-- Main Stylesheet -->
  <link href="css/style.css" rel="stylesheet">

  <!--Favicon-->
  <link rel="shortcut icon" href="images/favicon.jpg" type="image/x-icon">
  <link rel="icon" href="images/favicon.jpg" type="image/x-icon">

</head>

<body>
  <!-- preloader start --
  <div class="preloader">
    <img src="images/hcsi-pre.gif" alt="preloader" width="500px">
  </div>
  <!-- preloader end -->

<!-- header -->
<header class="fixed-top header">
  <!-- navbar -->
  <div class="navigation w-100">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark p-0">
        <!-- logo -->
        <a class="navbar-brand" href="index.html"><img src="images/logo.png" alt="logo" width="250"></a>
        <button class="navbar-toggler rounded-0" type="button" data-toggle="collapse" data-target="#navigation" aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <!-- menu -->
        <div class="collapse navbar-collapse" id="navigation">
          <ul class="navbar-nav ml-auto text-center">
            <!-- about -->
            <li class="nav-item dropdown view @@about">
              <a class="nav-link dropdown-toggle" href="#" id="navbarAbout" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                About
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarAbout">
                <a class="dropdown-item" href="labintro.html">Introduction</a>
                <a class="dropdown-item" href="researches.html">Research Areas</a>
                <a class="dropdown-item" href="collaborators.html">Collaborators</a>
              </div>
            </li>
            <!-- members -->
            <li class="nav-item dropdown view @@members">
              <a class="nav-link dropdown-toggle" href="#" id="navbarMembers" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Members
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarMembers">
                <a class="dropdown-item" href="zywu.html">Director</a>
                <a class="dropdown-item" href="members.html">Students</a>
                <a class="dropdown-item" href="alumni.html">Alumni</a>
                <a class="dropdown-item" href="honors.html">Honors</a>
              </div>
            </li>
            <!-- news -->
            <li class="nav-item @@news">
              <a class="nav-link" href="news.html">News</a>
            </li>
            <!-- research -->
            <li class="nav-item dropdown view active">
              <a class="nav-link dropdown-toggle" href="#" id="navbarResearch" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Research
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarResearch">
                <a class="dropdown-item active" href="publications.html">Publications</a>
                <a class="dropdown-item" href="patents.html">Patents</a>
                <a class="dropdown-item" href="projects.html">Projects</a>
              </div>
            </li>
            <!-- awards -->
            <li class="nav-item @@awards">
              <a class="nav-link" href="awards.html">Awards</a>
            </li>
            <!-- demos -->
            <li class="nav-item @@demos">
              <a class="nav-link" href="demos.html">Demos</a>
            </li>
          </ul>
        </div>
        <!-- /menu -->
      </nav>
    </div>
  </div>
  <!-- /navbar -->
</header>
<!-- /header -->

<!-- page title -->
<section class="page-title-section overlay" data-background="images/backgrounds/hcsi-gather.jpg">
  <div class="container">
    <div class="row">
      <div class="col-md-8">
        <ul class="list-inline custom-breadcrumb">
          <li class="list-inline-item"><p class="h2 text-primary font-secondary">Publications</p></li>
          <li class="list-inline-item text-white h3 font-secondary"></li>
        </ul>
        <p class="text-lighten">&nbsp;</p>
      </div>
    </div>
  </div>
</section>
<!-- /page title -->

<!-- papers -->
<section class="section">
  <div class="container">
    <!-- paper category navigation bar -->
    <div class="row">
      <div class="col-12">
        <!-- nav tab -->
        <div class="border-bottom">
          <ul class="nav nav-pills text-center">
            <li class="navi-item">
              <a class="nav-link active" href="#year" data-toggle="pill">
                  by year
              </a>
            </li>
            <li class="navi-item">
              <a class="nav-link" href="#area" data-toggle="pill">
                  by area
              </a>
            </li>
            <li class="navi-item">
              <a class="nav-link" href="#cat" data-toggle="pill">
                  by category
              </a>
            </li>
          </ul>
        </div>
        <!-- nav tab content -->
        <div class="tab-content" id="pills-tabContent">
          <div class="tab-pane fade show active" id="year">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2022">2022</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2021">2021</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2020">2020</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2019">2019</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2018">2018</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2017">2017</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2016">2016</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2015">2015</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2014">2014</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2013">2013</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2012">2012</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2011">2011</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2010">2010</li>
                <li class="list-inline-item px-2 mb-3" data-filter="2009">2009</li>
              </ul>
            </div>
          </div>
          <div class="tab-pane fade" id="area">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ss">Speech Synthesis</li>
                <li class="list-inline-item px-2 mb-3" data-filter="sr">Speech Recognition</li>
                <li class="list-inline-item px-2 mb-3" data-filter="sv">Speaker Recognition</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ssp">Speech Signal Processing</li>
                <li class="list-inline-item px-2 mb-3" data-filter="ac">Affective Computing</li>
                <li class="list-inline-item px-2 mb-3" data-filter="mslp">Multimodal Speech and Language Processing</li>
              </ul>
            </div>
          </div>
          <div class="tab-pane fade" id="cat">
            <div class="col-12">
              <div class="mb-3"></div>
              <!-- paper category list -->
              <ul class="list-inline text-center filter-controls mb-3">
                <li class="list-inline-item px-2 mb-3 active" data-filter="all">All</li>
                <li class="list-inline-item px-2 mb-3" data-filter="jnl">Journal</li>
                <li class="list-inline-item px-2 mb-3"  data-filter="cf">Conference</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- papers list -->
    <div class="filtr-container">
      <!-- paper -->
      <div data-category="2022,jnl,sv" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Haibin Wu, Xu Li, Andy T Liu, Zhiyong Wu, Helen Meng, Hung-Yi Lee,
            "Improving the Adversarial Robustness for Speaker Verification by Self-supervised Learning,"
            <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>,
            vol. 30, pp. 202-217. IEEE, January, 2022.
            <span class="text-lighten">(SCI: WOS:000742179300004, EI: 20215111368713, THU-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/document/9645217">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Shiyin Kang, Helen Meng,
            "Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7922-7926. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312199470, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747438">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-expressive-tts-hierarchical-context/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jingbei Li, Yi Meng, Chenyi Li, Zhiyong Wu, Helen Meng, Chao Weng, Dan Su,
            "Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-Based Multi-Modal Context Modeling,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7917-7921. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747837">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-conversational-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Liyang Chen, Zhiyong Wu, Jun Ling, Runnan Li, Xu Tan, Sheng Zhao,
            "Transformer-S2A: Robust and Efficient Speech-to-Animation,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7247-7251. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198574, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747495">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-Transformer-S2A/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xintao Zhao, Feng Liu, Changhe Song, Zhiyong Wu, Shiyin Kang, Deyi Tuo, Helen Meng,
            "Disentangling Content and Fine-Grained Prosody Information Via Hybrid ASR Bottleneck Features for Voice Conversion,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7022-7026. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198907, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747625">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-hybrid-bottleneck-vc/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xueyuan Chen, Changhe Song, Yixuan Zhou, Zhiyong Wu, Changbin Chen, Zhongqin Wu, Helen Meng,
            "A Character-Level Span-Based Model for Mandarin Prosodic Structure Prediction,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7602-7606. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198495, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747315">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/SpanPSP">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/SpanPSP/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Wenlin Dai, Changhe Song, Xiang Li, Zhiyong Wu, Huashan Pan, Xiulin Li, Helen Meng,
            "An End-to-End Chinese Text Normalization Model Based on Rule-Guided Flat-Lattice Transformer,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7122-7126. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198496, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747316">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/FlatTN">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ss,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jingbei Li, Yi Meng, Zhiyong Wu, Helen Meng, Qiao Tian, Yuping Wang, Yuxuan Wang,
            "Neufa: Neural Network Based End-to-End Forced Alignment with Bidirectional Attention Mechanism,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 8007-8011. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198218, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747085">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/NeuFA">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Wenxuan Ye, Shaoguang Mao, Frank Soong, Wenshan Wu, Yan Xia, Jonathan Tien, Zhiyong Wu,
            "An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6827-6831. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312199246, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746604">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2022-MDD-APL/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ssp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, Helen Meng,
            "FullSubNet+: Channel Attention Fullsubnet with Complex Spectrograms for Speech Enhancement,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7857-7861. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9747888">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/FullSubNet-plus">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://hit-thusz-rookiecj.github.io/fullsubnet-plus.github.io/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin Wu, Shoukang Hu, Zhiyong Wu, Xunying Liu, Helen Meng,
            "Neural Architecture Search for Speech Emotion Recognition,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6902-6906. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198129, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746155">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2022,cf,sv" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Haibin Wu, Po-Chun Hsu, Ji Gao, Shanshan Zhang, Shen Huang, Jian Kang, Zhiyong Wu, Helen Meng, Hung-Yi Lee,
            "Adversarial Sample Detection for Speaker Verification by Neural Vocoders,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 236-240. Singapore, May 22-27, 2022.
            <span class="text-lighten">(EI: 20222312198990, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9746900">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/hbwu-ntu/spot-adv-by-vocoder">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,jnl,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin Wu, Yuewen Cao, Hui Lu, Songxiang Liu, Disong Wang, Zhiyong Wu, Xunying Liu, Helen Meng,
            "Speech Emotion Recognition Using Sequential Capsule Networks,"
            <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>,
            vol. 29, pp. 3280-3291. IEEE, October, 2021.
            <span class="text-lighten">(SCI: WOS:000714713700004, EI: 20214311082562, THU-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9576634">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,jnl,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xixin Wu, Yuewen Cao, Hui Lu, Songxiang Liu, Shiyin Kang, Zhiyong Wu, Xunying Liu, Helen Meng,
            "Exemplar-Based Emotive Speech Synthesis,"
            <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>,
            vol. 29, pp. 874-886. IEEE, January, 2021.
            <span class="text-lighten">(SCI: WOS:000619310400001, EI: 20210409830187, THU-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9328288">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www1.se.cuhk.edu.hk/~wuxx/TASLP/ExemplarTTS.html">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yingmei Guo, Linjun Shou, Jian Pei, Ming Gong, Mingxing Xu, Zhiyong Wu, Daxin Jiang,
            "Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding,"
            [in] <i>Proc. 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>,
            pp. 1-12. Punta Cana, Dominican Republic, November 7-11, 2021.
            <span class="text-lighten">(EI: 20221411909706, THU-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2109.01583">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yaohua Bu, Tianyi Ma, Weijun Li, Hang Zhou, Jia Jia, Shengqi Chen, Kaiyuan Xu, Dachuan Shi, Haozhe Wu, Zhihan Yang, Kun Li, Zhiyong Wu, Yuanchun Shi, Xiaobo Lu, Ziwei Liu,
            "PTeacher: A Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback,"
            [in] <i>Proc. 2021 CHI Conference on Human Factors in Computing Systems (CHI)</i>,
            pp. 1-14. Yokohama, Japan, May 8-13, 2021.
            <span class="text-lighten">(EI: 20212210439123, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/3411764.3445490">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://hcsi.cs.tsinghua.edu.cn/demo/CHI21-BUYAOHUA.mp4">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Suping Zhou, Jia Jia, Zhiyong Wu, Zhihan Yang, Yanfeng Wang, Wei Chen, Fanbo Meng, Shuo Huang, Jialie Shen, Xiaochuan Wang,
            "Inferring Emotion from Large-Scale Internet Voice Data: A Semi-supervised Curriculum Augmentation based Deep Learning Approach,"
            [in] <i>Proc. the 35th AAAI Conference on Artificial Intelligence (AAAI)</i>,
            pp. 6039-6047. Virtual, Online, February 2-9, 2021.
            <span class="text-lighten">(EI: 20222012114882, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ojs.aaai.org/index.php/AAAI/article/view/16753">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Liangqi Liu, Jiankun Hu, Zhiyong Wu, Song Yang, Songfan Yang, Jia Jia, Helen Meng,
            "Controllable Emphatic Speech Synthesis based on Forward Attention for Expressive Speech Synthesis,"
            [in] <i>Proc. IEEE Spoken Language Technology Workshop (SLT)</i>,
            pp. 410-414. Shenzhen, China, January 19-22, 2021.
            <span class="text-lighten">(EI: 20211510210781, <font color="#FF0000">Best Paper Finalist</font>)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9383537">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/slt2021-controllable-emphasis-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Huirong Huang, Zhiyong Wu, Shiyin Kang, Dongyang Dai, Jia Jia, Tianxiao Fu, Deyi Tuo, Guangzhi Lei, Peng Liu, Dan Su, Dong Yu, Helen Meng,
            "Speaker Independent and Multilingual/Mixlingual Speech-driven Talking Head Generation Using Phonetic Posteriorgrams,"
            [in] <i>Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</i>,
            pp. 1433-1437. Tokyo, Japan, December 14-17, 2021.
            <span class="text-lighten">(EI: 20221211827369)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9689472">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/apsipa2021-talking-head-samples/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Aolan Sun, Jianzong Wang, Ning Cheng, Methawee Tantrawenith, Zhiyong Wu, Helen Meng, Edward Xiao, Jing Xiao,
            "Reconstructing Dual Learning for Neural Voice Conversion Using Relatively Few Samples,"
            [in] <i>Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</i>,
            pp. 946-953. December 13-17, 2021.
            <span class="text-lighten">(EI: 20221211830976)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9687965">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ssp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xinyu Cai, Heinrich Dinkel, Zhiyong Yan, Yongqing Wang, Junbo Zhang, Zhiyong Wu, Yujun Wang,
            "A Contrastive Semi-Supervised Learning Framework For Anomaly Sound Detection,"
            [in] <i>Proc. Workshop on Detection and Classification of Acousitic Scenes and Events (DCASE)</i>,
            pp. 31-34. November 15â€“19, 2021.
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Cai_16.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/bibiaaaa/SmallRice_DCASE2021Challenge">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dcase.community/documents/workshop2021/posters/DCASE2021Workshop_Cai_16-poster.pdf">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Hui Lu, Zhiyong Wu, Xixin Wu, Xu Li, Shiyin Kang, Xunying Liu, Helen Meng,
            "VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis,"
            [in] <i>Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>,
            pp. 3775-3779. Brno, Czech republic, August 30-September 3, 2021.
            <span class="text-lighten">(EI: 20214711186915, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2107.03298">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/VAENAR-TTS">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://light1726.github.io/vaenar-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xiang Li, Changhe Song, Jingbei Li, Zhiyong Wu, Jia Jia, Helen Meng,
            "Towards Multi-Scale Style Control for Expressive Speech Synthesis,"
            [in] <i>Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>,
            pp. 4673-4677. Brno, Czech republic, August 30-September 3, 2021.
            <span class="text-lighten">(EI: 20214711190435, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2104.03521">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/interspeech2021-multi-scale-style-control/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jie Wang, Jingbei Li, Xintao Zhao, Zhiyong Wu, Shiyin Kang, Helen Meng,
            "Adversarially Learning Disentangled Speech Representations for Robust Multi-factor Voice Conversion,"
            [in] <i>Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>,
            pp. 846-850. Brno, Czech republic, August 30-September 3, 2021.
            <span class="text-lighten">(EI: 20214711194412, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2102.00184">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/interspeech2021-multi-factor-vc/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,sv" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Haibin Wu, Yang Zhang, Zhiyong Wu, Dong Wang, Hung-Yi Lee,
            "Voting for the Right Answer: Adversarial Defense for Speaker Verification,"
            [in] <i>Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>,
            pp. 4294-4298. Brno, Czech republic, August 30-September 3, 2021.
            <span class="text-lighten">(EI: 20214711194533, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://arxiv.org/abs/2106.07868">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/adsv_voting">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://zyzisyz.github.io/voting_audio_samples/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan Su, Helen Meng,
            "Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 5894-5898. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810913803, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9414694">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Changhe Song, Jingbei Li, Yixuan Zhou, Zhiyong Wu, Helen Meng,
            "Syntactic Representation Learning for Neural Network based TTS with Syntactic Parse Tree Traversal,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6064-6068. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810921439, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9414671">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2021-tree-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xiong Cai, Dongyang Dai, Zhiyong Wu, Xiang Li, Jingbei Li, Helen Meng,
            "Emotion Controllable Speech Synthesis using Emotion-Unlabeled Dataset with the Assistance of Cross-Domain Speech Emotion Recognition,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 5734-5738. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810922222, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9413907">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/icassp2021-emotion-tts">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://thuhcsi.github.io/icassp2021-emotion-tts/">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jie Wnag, Yuren You, Feng Liu, Deyi Tuo, Shiyin Kang, Zhiyong Wu, Helen Meng,
            "The Huya Multi-speaker and Multi-style Speech Synthesis System for M2VOC Challenge 2020,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 8608-8612. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810913901, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9414943">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,sv" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Haibin Wu, Xu Li, Andy T. Liu, Zhiyong Wu, Helen Meng, Hung-Yi Lee,
            "Adversarial Defense for Automatic Speaker Verification by Cascaded Self-supervised Learning Models,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 6718-6722. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810914628, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9413737">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Bin Su, Shaoguang Mao, Frank Soong, Yan Xia, Jonathan Tien, Zhiyong Wu,
            "Improving Pronunciation Assessment via Ordinal Regression with Anchored Reference Samples,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 7748-7752. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810908107, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9413659">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Qicong Xie, Xiaohai Tian, Guanghou Liu, Kun Song, Lei Xie, Zhiyong Wu, Hai Li, Song Shi, Haizhou Li, Fen Hong, Hui Bu, Xin Xu,
            "The Multi-speaker Multi-style Voice Cloning Challenge 2021,"
            [in] <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>,
            pp. 8613-8617. Toronto, Canada, June 6-11, 2021.
            <span class="text-lighten">(EI: 20213810922367, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9414001">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,ac" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Xiong Cai, Zhiyong Wu, Kuo Zhong, Bin Su, Dongyang Dai, Helen Meng,
            "Unsupervised Cross-Lingual Speech Emotion Recognition Using Domain Adversarial Neural Network,"
            [in] <i>Proc. International Symposium on Chinese Spoken Language Processing (ISCSLP)</i>,
            pp. 1-5. Hong Kong, China, January 24-26, 2021.
            <span class="text-lighten">(EI: 20211210098767)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9362058">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2021,cf,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Michael Lao BanTeng, Zhiyong Wu,
            "Channel-Wise Dense Connection Graph Convolutional Network for Skeleton-Based Action Recognition,"
            [in] <i>Proc. International Conference on Pattern Recognition (ICPR)</i>,
            pp. 3799-3806. Milan, Italy, January 10-15, 2021.
            <span class="text-lighten">(EI: 20212910658234, THU-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/9412329">Paper</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2019,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Runnan LI, Zhiyong WU, Jia JIA, Yaohua BU, Sheng ZHAO, Helen MENG,
            "Towards Discriminative Representation Learning for Speech Emotion Recognition,"
            [in] <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI)</i>,
            pp. 5060-5066. Macao, China, August 10-16, 2019.
            <span class="text-lighten">(EI: 20194607696464, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.ijcai.org/proceedings/2019/0703.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://github.com/thuhcsi/IJCAI2019-DRL4SER">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2019,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yishuang NING, Sheng HE, Zhiyong WU, Chunxiao XING, Liangjie ZHANG,
            "A Review of Deep Learning Based Speech Synthesis,"
            <i>Applied Sciences-Basel</i>,
            vol. 9, no. 19, pp. 4050. MDPI, September, 2019.
            <span class="text-lighten">(SCI: WOS:000496258100108)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.mdpi.com/2076-3417/9/19/4050">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2018,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Runnan LI, Zhiyong WU, Jia JIA, Jingbei LI, Wei CHEN, Helen MENG,
            "Inferring User Emotive State Changes in Realistic Human-Computer Conversational Dialogs,"
            [in] <i>Proc. ACM Multimedia Conference (ACM MM)</i>,
            pp. 136-144. Seoul, Korea, October 22-26, 2018.
            <span class="text-lighten">(EI: 20185006246269, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/3240508.3240575">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2018,jnl,sr" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Kun LI, Shaoguang MAO, Xu LI, Zhiyong WU, Helen MENG,
            "Automatic Lexical Stress and Pitch Accent Detection for L2 English Speech using Multi-Distribution Deep Neural Networks,"
            <i>Speech Communication (Speech Com)</i>,
            vol. 96, pp. 28-36. Elsevier, February, 2018.
            <span class="text-lighten">(SCI: WOS:000424723700003, EI: 20174704448303, CCF-B)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.sciencedirect.com/science/article/pii/S0167639315300637">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2017,cf,ac,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Yishuang NING, Jia JIA, Zhiyong WU, Runnan LI, Yongsheng AN, Yanfeng WANG, Helen MENG,
            "Multi-task Deep Learning for User Intention Understanding in Speech Interaction Systems,"
            [in] <i>Proc. the 31th AAAI Conference on Artificial Intelligence (AAAI)</i>,
            pp. 161-167. San Francisco, USA, February 4-9, 2017.
            <span class="text-lighten">(EI: 20174104242835, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ojs.aaai.org/index.php/AAAI/article/view/10493">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Yishuang NING, Xiao ZANG, Jia JIA, Fanbo MENG, Helen MENG, Lianhong CAI,
            "Generating Emphatic Speech with Hidden Markov Model for Expressive Speech Synthesis,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 74, no. 22, pp. 9909-9925. Springer, July, 2015.
             <span class="text-lighten">(SCI: WOS:000364019400005, EI: 20143600027913, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-014-2164-2">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Kai ZHAO, Xixin WU, Xinyu LAN, Helen MENG,
            "Acoustic to Articulatory Mapping with Deep Neural Network,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 74, no. 22, pp. 9889-9907. Springer, August, 2015.
            <span class="text-lighten">(SCI: WOS:000364019400004, EI: 20143600014973, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-014-2183-z">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,cf,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Qi LYU, Zhiyong WU, Jun ZHU,
            "Polyphonic Music Modelling with LSTM-RTRBM,"
            [in] <i>Proc. ACM Multimedia Conference (ACM MM)</i>,
            pp. 991-994. Brisbane, Australia, October 26-30, 2015.
            <span class="text-lighten">(EI: 20161602252616, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://dl.acm.org/doi/abs/10.1145/2733373.2806383">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2015,cf,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Qi LYU, Zhiyong WU, Jun ZHU, Helen MENG,
            "Modelling High-dimensional Sequences with LSTM-RTRBM: Application to Polyphonic Music Generation,"
            [in] <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI)</i>,
            pp. 4138-4139. Buenos Aires, Argentina, July 25-31, 2015.
            <span class="text-lighten">(EI: 20155101693661, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://www.ijcai.org/Proceedings/15/Papers/582.pdf">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2014,jnl,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Fanbo MENG, Zhiyong WU, Jia JIA, Helen MENG, Lianhong CAI,
            "Synthesizing English Emphatic Speech for Multimodal Corrective Feedback in Computer-Aided Pronunciation Training,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 73, no. 1, pp. 463-489. Springer, September, 2014.
            <span class="text-lighten">(SCI: WOS:000342418700022, EI: 20143600046713, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-013-1601-y">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2014,jnl,ss,mslp" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Jia JIA, Zhiyong WU, Shen ZHANG, Helen MENG, Lianhong CAI,
            "Head and Facial Gestures Synthesis using PAD Model for an Expressive Talking Avatar,"
            <i>Multimedia Tools and Applications (MTA)</i>,
            vol. 73, no. 1, pp. 439-461. Springer, September, 2014.
            <span class="text-lighten">(SCI: WOS:000342418700023, EI: 20143600046670, CCF-C)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://link.springer.com/article/10.1007/s11042-013-1604-8">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- paper -->
      <div data-category="2009,jnl,ac,ss" class="col-12 filtr-item">
        <div class="card border-0 rounded-0 hover-shadow bg-gray p-2 mb-2">
          <div class="text-dark">
            Zhiyong WU, Helen M. MENG, Hongwu YANG, Lianhong CAI,
            "Modeling the Expressivity of Input Text Semantics for Chinese Text-to-Speech Synthesis in a Spoken Dialog System,"
            <i>IEEE Transaction on Audio, Speech and Language Processing (TASLP)</i>,
            vol. 17, no. 8, pp. 1567-1577. IEEE, November, 2009.
            <span class="text-lighten">(SCI: WOS:000268903600010, EI: 20093612281690, CCF-A)</span>
            <ul class="list-inline bg-gray mt-1">
              <li class="list-inline-item"><a class="d-inline-block text-light" href="https://ieeexplore.ieee.org/abstract/document/4926212">Paper</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Code</a></li>
              <li class="list-inline-item"><a class="d-inline-block text-light">Demo</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- /papers -->

<!-- footer -->
<footer>
  <!-- footer content -->
  <div class="footer bg-footer section border-bottom">
    <div class="container">
      <div class="row">
        <div class="col-lg-4 col-md-4 col-sm-5 mb-5 mb-sm-0 text-center">
          <!-- logo -->
          <a class="logo-footer" href="index.html"><img class="img-fluid mb-0" src="images/QRCode.png" alt="QR code"></a>
        </div>
        <!-- contact -->
        <div class="col-lg-8 col-md-8 col-sm-7 mb-0">
          <h4 class="text-lightblue mb-2">Location</h4>
          <p class="text-white mb-4">Room 1701, Information Building, Tsinghua Campus, The University Town, Shenzhen 518055, China <br>
            æ·±åœ³å¸‚å—å±±åŒºè¥¿ä¸½å¤§å­¦åŸŽæ¸…åŽæ ¡åŒºä¿¡æ¯å¤§æ¥¼1701
          </p>
          <h4 class="text-lightblue mb-2">Follow Us</h4>
          <p class="text-white mb-0">Scan QR code to follow us on WeChat <br>
            æ‰«ç å…³æ³¨å®žéªŒå®¤å¾®ä¿¡å…¬ä¼—å·
          </p>
        </div>
      </div>
    </div>
  </div>
  <!-- copyright -->
  <div class="copyright py-4 bg-footer">
    <div class="container">
      <div class="row">
        <div class="col-sm-7 text-sm-left text-center">
          <p class="mb-0">Copyright &copy;
            <script>
              var CurrentYear = new Date().getFullYear()
              document.write(CurrentYear)
            </script>,
            im1eon @ thuhcsi</p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!-- /footer -->

<!-- jQuery -->
<script src="plugins/jQuery/jquery.min.js"></script>
<!-- Bootstrap JS -->
<script src="plugins/bootstrap/bootstrap.min.js"></script>
<!-- slick slider -->
<script src="plugins/slick/slick.min.js"></script>
<!-- aos -->
<script src="plugins/aos/aos.js"></script>
<!-- venobox popup -->
<script src="plugins/venobox/venobox.min.js"></script>
<!-- filter -->
<script src="plugins/filterizr/jquery.filterizr.min.js"></script>

<!-- Main Script -->
<script src="js/script.js"></script>

</body>
</html>
